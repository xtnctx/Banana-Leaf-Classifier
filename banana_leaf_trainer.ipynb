{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install dependencies\n",
        "\n",
        "This may take a long time ... ~ 20 mins\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T6MzSkp0y435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.10.0-cp38-cp38-win_amd64.whl (455.9 MB)\n",
            "Collecting tensorflow-gpu\n",
            "  Using cached tensorflow_gpu-2.10.0-cp38-cp38-win_amd64.whl (455.9 MB)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.6.0-cp38-cp38-win_amd64.whl (7.2 MB)\n",
            "Collecting tabulate\n",
            "  Using cached tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: setuptools in c:\\users\\bahillo\\desktop\\simbi\\venv\\lib\\site-packages (from tensorflow->-r requirements.txt (line 1)) (49.2.1)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting h5py>=2.9.0\n",
            "  Using cached h5py-3.7.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
            "Requirement already satisfied: packaging in c:\\users\\bahillo\\desktop\\simbi\\venv\\lib\\site-packages (from tensorflow->-r requirements.txt (line 1)) (21.3)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.49.1-cp38-cp38-win_amd64.whl (3.6 MB)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Using cached flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.27.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Using cached tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "Collecting keras<2.11,>=2.10.0\n",
            "  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Collecting typing-extensions>=3.6.6\n",
            "  Using cached typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting libclang>=13.0.0\n",
            "  Using cached libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
            "Collecting numpy>=1.20\n",
            "  Using cached numpy-1.23.3-cp38-cp38-win_amd64.whl (14.7 MB)\n",
            "Collecting tensorboard<2.11,>=2.10\n",
            "  Using cached tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\bahillo\\desktop\\simbi\\venv\\lib\\site-packages (from tensorflow->-r requirements.txt (line 1)) (1.16.0)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Using cached protobuf-3.19.5-cp38-cp38-win_amd64.whl (896 kB)\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Using cached termcolor-2.0.1-py3-none-any.whl (5.4 kB)\n",
            "Collecting absl-py>=1.0.0\n",
            "  Using cached absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Using cached wrapt-1.14.1-cp38-cp38-win_amd64.whl (35 kB)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Using cached fonttools-4.37.3-py3-none-any.whl (959 kB)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Using cached contourpy-1.0.5-cp38-cp38-win_amd64.whl (164 kB)\n",
            "Collecting cycler>=0.10\n",
            "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Using cached kiwisolver-1.4.4-cp38-cp38-win_amd64.whl (55 kB)\n",
            "Collecting pillow>=6.2.0\n",
            "  Using cached Pillow-9.2.0-cp38-cp38-win_amd64.whl (3.3 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bahillo\\desktop\\simbi\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\bahillo\\desktop\\simbi\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n",
            "Collecting wheel<1.0,>=0.23.0\n",
            "  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "Collecting requests<3,>=2.21.0\n",
            "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
            "Collecting google-auth<3,>=1.6.3\n",
            "  Using cached google_auth-2.11.1-py2.py3-none-any.whl (167 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
            "  Using cached importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2022.9.14-py3-none-any.whl (162 kB)\n",
            "Collecting charset-normalizer<3,>=2\n",
            "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting MarkupSafe>=2.1.1\n",
            "  Using cached MarkupSafe-2.1.1-cp38-cp38-win_amd64.whl (17 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
            "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Using cached oauthlib-3.2.1-py3-none-any.whl (151 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Using cached zipp-3.8.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Installing collected packages: wheel, astunparse, numpy, h5py, grpcio, flatbuffers, gast, tensorflow-io-gcs-filesystem, opt-einsum, tensorflow-estimator, keras, typing-extensions, keras-preprocessing, libclang, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, urllib3, certifi, charset-normalizer, idna, requests, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, absl-py, zipp, importlib-metadata, markdown, tensorboard-data-server, MarkupSafe, werkzeug, protobuf, tensorboard, google-pasta, termcolor, wrapt, tensorflow, tensorflow-gpu, fonttools, contourpy, cycler, kiwisolver, pillow, matplotlib, tabulate\n",
            "Successfully installed MarkupSafe-2.1.1 absl-py-1.2.0 astunparse-1.6.3 cachetools-5.2.0 certifi-2022.9.14 charset-normalizer-2.1.1 contourpy-1.0.5 cycler-0.11.0 flatbuffers-2.0.7 fonttools-4.37.3 gast-0.4.0 google-auth-2.11.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.49.1 h5py-3.7.0 idna-3.4 importlib-metadata-4.12.0 keras-2.10.0 keras-preprocessing-1.1.2 kiwisolver-1.4.4 libclang-14.0.6 markdown-3.4.1 matplotlib-3.6.0 numpy-1.23.3 oauthlib-3.2.1 opt-einsum-3.3.0 pillow-9.2.0 protobuf-3.19.5 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 tabulate-0.8.10 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-gpu-2.10.0 tensorflow-io-gcs-filesystem-0.27.0 termcolor-2.0.1 typing-extensions-4.3.0 urllib3-1.26.12 werkzeug-2.2.2 wheel-0.37.1 wrapt-1.14.1 zipp-3.8.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 20.2.1; however, version 22.2.2 is available.\n",
            "You should consider upgrading via the 'c:\\Users\\Bahillo\\Desktop\\Simbi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3NmZDq3Xs1H"
      },
      "source": [
        "## Upload data\n",
        "\n",
        "Group images by separating each class into one folder then wrap all the folder into another folder.\n",
        "\n",
        "Ex.\n",
        "```\n",
        "data/ \n",
        "  │\n",
        "  └─── class1/\n",
        "  │        │\n",
        "  |        └─── image1.png\n",
        "  │        └─── image2.jpg\n",
        "  |        └─── ...\n",
        "  │   \n",
        "  └─── class2/\n",
        "  │        │\n",
        "  |        └─── image123.png\n",
        "  │        └─── image456.jpg\n",
        "  |        └─── ...\n",
        "  |\n",
        "  └─── .../\n",
        "           │\n",
        "           └─── ...\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opf5mi7TaBkz"
      },
      "source": [
        "## Remove unecessary files\n",
        "\n",
        "Image file extension that are only acceptable are selected in the `image_exts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mjWJCi2sbIBn"
      },
      "outputs": [],
      "source": [
        "import imghdr, os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pYsl0Bo8YXi1"
      },
      "outputs": [],
      "source": [
        "data_dir = './Banana Leaf Disease'\n",
        "image_exts = ['jpeg', 'jpg', 'bmp', 'png']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pyQaeAa2a14o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed 0 unwanted files.\n"
          ]
        }
      ],
      "source": [
        "nr = 0\n",
        "for image_class in os.listdir(data_dir):\n",
        "  for image in os.listdir(os.path.join(data_dir, image_class)):\n",
        "    image_path = os.path.join(data_dir, image_class, image)\n",
        "    try:\n",
        "      tip = imghdr.what(image_path)\n",
        "      if tip not in image_exts:\n",
        "        print(f'Image not in ext list {image_path}')\n",
        "        os.remove(image_path)\n",
        "        nr += 1\n",
        "    except Exception as e:\n",
        "        print(f'Issue with image {image_path}')\n",
        "\n",
        "print(f'Removed {nr} unwanted files.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure settings\n",
        "\n",
        "Apply all the settings here for preprocessing, building, and training the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (256, 256) # Square sized are recommended for stability\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = ...\n",
        "KERNEL_SIZE = (3, 3)\n",
        "STRIDES = 1\n",
        "\n",
        "# Ratio for splitting dataset\n",
        "TRAIN_VAL = 0.7\n",
        "VALID_VAL = 0.2\n",
        "TEST_VAL = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzGOzfkSc0Fl"
      },
      "source": [
        "## Prepare, randomize, and normalize the images\n",
        "\n",
        "Using the `tf.keras.utils.image_dataset_from_directory` with image_size of (256, 256).\n",
        "\n",
        "See https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_RTO-B8jfZ3v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TTgo-RYGfR_X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 268 files belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "data = tf.keras.utils.image_dataset_from_directory(data_dir, image_size=IMAGE_SIZE, shuffle=True) # Images are resized and shuffled\n",
        "class_names = data.class_names\n",
        "data = data.map(lambda x, y : (x/255, y)) # Normalize data between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 256, 256, 3)\n",
            "(32,)\n"
          ]
        }
      ],
      "source": [
        "for image_batch, labels_batch in data:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MPdkFJ7gcRQ"
      },
      "source": [
        "## Split the input and output pairs for training\n",
        "\n",
        "Randomly split input and output pairs into sets of data: 70% for training, 20% for validation, and 10% for testing.\n",
        "\n",
        "  - the training set is used to train the model\n",
        "  - the validation set is used to measure how well the model is performing during training\n",
        "  - the testing set is used to test the model after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hZUelmtUm6H-"
      },
      "outputs": [],
      "source": [
        "train_size = round(len(data)*TRAIN_VAL)\n",
        "val_size = round(len(data)*VALID_VAL)\n",
        "test_size = round(len(data)*TEST_VAL)\n",
        "assert train_size + val_size + test_size == len(data), f'sum must be {len(data)}'\n",
        "\n",
        "train = data.take(train_size)\n",
        "val = data.skip(train_size).take(val_size)\n",
        "test = data.skip(train_size + val_size).take(test_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure the dataset for performance\n",
        "\n",
        "* [Dataset.cache](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
        "\n",
        "* [Dataset.prefetch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) overlaps data preprocessing and model execution while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val = val.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test = test.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8M7urx0srNR"
      },
      "source": [
        "## Build & Train the Model\n",
        "\n",
        "Build and train a [TensorFlow](https://www.tensorflow.org) model using the high-level [Keras](https://www.tensorflow.org/guide/keras) API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqWbwfAMu6h-"
      },
      "source": [
        "### Build the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BJGqDkXMs17t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 256, 256, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 128, 128, 16)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 128, 128, 32)      4640      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 64, 64, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 64, 64, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 32, 32, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 65536)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               8388736   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4)                 516       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,412,836\n",
            "Trainable params: 8,412,836\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(16, KERNEL_SIZE, STRIDES, padding='same', activation='relu', input_shape=IMAGE_SIZE+(3,)))\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(32, KERNEL_SIZE, STRIDES, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64, KERNEL_SIZE, STRIDES, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile('adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUzIcUJ-u0p2"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lkG48YX87-b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "6/6 [==============================] - 10s 2s/step - loss: 3.1518 - accuracy: 0.3177 - val_loss: 1.2786 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.1243 - accuracy: 0.4844 - val_loss: 0.9039 - val_accuracy: 0.6094\n",
            "Epoch 3/20\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.8479 - accuracy: 0.6615 - val_loss: 0.7451 - val_accuracy: 0.7031\n",
            "Epoch 4/20\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.6954 - accuracy: 0.7708 - val_loss: 0.6423 - val_accuracy: 0.7656\n",
            "Epoch 5/20\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.4625 - accuracy: 0.8229 - val_loss: 0.6735 - val_accuracy: 0.6719\n",
            "Epoch 6/20\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.4951 - accuracy: 0.8021 - val_loss: 0.5281 - val_accuracy: 0.7656\n",
            "Epoch 7/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.3348 - accuracy: 0.8958 - val_loss: 0.3929 - val_accuracy: 0.8281\n",
            "Epoch 8/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.2544 - accuracy: 0.9010 - val_loss: 0.2678 - val_accuracy: 0.9375\n",
            "Epoch 9/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.1436 - accuracy: 0.9479 - val_loss: 0.2084 - val_accuracy: 0.9531\n",
            "Epoch 10/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.1475 - accuracy: 0.9583 - val_loss: 0.1469 - val_accuracy: 0.9531\n",
            "Epoch 11/20\n",
            "6/6 [==============================] - 12s 2s/step - loss: 0.1746 - accuracy: 0.9427 - val_loss: 0.2555 - val_accuracy: 0.9375\n",
            "Epoch 12/20\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.1180 - accuracy: 0.9688 - val_loss: 0.1764 - val_accuracy: 0.9688\n",
            "Epoch 13/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.0652 - accuracy: 0.9896 - val_loss: 0.3192 - val_accuracy: 0.9375\n",
            "Epoch 14/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.0658 - accuracy: 0.9844 - val_loss: 0.3511 - val_accuracy: 0.9531\n",
            "Epoch 15/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.0548 - accuracy: 0.9844 - val_loss: 0.1586 - val_accuracy: 0.9531\n",
            "Epoch 16/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.0522 - accuracy: 0.9792 - val_loss: 0.2106 - val_accuracy: 0.9375\n",
            "Epoch 17/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.0481 - accuracy: 0.9896 - val_loss: 0.1911 - val_accuracy: 0.9531\n",
            "Epoch 18/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.0380 - accuracy: 0.9896 - val_loss: 0.2971 - val_accuracy: 0.9688\n",
            "Epoch 19/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.4087 - val_accuracy: 0.9531\n",
            "Epoch 20/20\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.0243 - accuracy: 0.9948 - val_loss: 0.1183 - val_accuracy: 0.9688\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train, epochs=20, validation_data=val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uRAHMkGvPb-"
      },
      "source": [
        "## Evaluate with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 335ms/step\n",
            "+---------------+------------+----------+\n",
            "|   Predictions |   Expected | Result   |\n",
            "|---------------+------------+----------|\n",
            "|             1 |          1 | True     |\n",
            "|             3 |          3 | True     |\n",
            "|             3 |          3 | True     |\n",
            "|             0 |          0 | True     |\n",
            "|             1 |          1 | True     |\n",
            "|             0 |          0 | True     |\n",
            "|             3 |          3 | True     |\n",
            "|             2 |          0 | False    |\n",
            "|             3 |          3 | True     |\n",
            "|             1 |          1 | True     |\n",
            "|             0 |          0 | True     |\n",
            "|             1 |          1 | True     |\n",
            "+---------------+------------+----------+\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.3526 - accuracy: 0.9167\n",
            "Model loss (Test set): 0.35258087515830994\n",
            "Model Accuracy (Test set): 0.9166666865348816\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "points = test.map(lambda x, y: x)\n",
        "labels = test.map(lambda x, y: y)\n",
        "test_yhat = model.predict(points)\n",
        "test_yhat = tf.math.argmax(test_yhat, -1)\n",
        "\n",
        "# print the predictions and the expected ouputs\n",
        "result = list(zip(np.array(test_yhat), list(labels.as_numpy_iterator())[0]))\n",
        "delta = [True if elem[0] == elem[1] else False for elem in result]\n",
        "table = list(zip(*zip(*result), delta))\n",
        "\n",
        "print(tabulate(table, headers=[\"Predictions\", \"Expected\", \"Result\"], tablefmt=\"psql\"))\n",
        "\n",
        "loss, acc = model.evaluate(test)\n",
        "print(f'Model loss (Test set): {loss}')\n",
        "print(f'Model Accuracy (Test set): {acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLHwnChCxoco"
      },
      "source": [
        "## Save the model\n",
        "\n",
        " To test different brand new images that are not in the `data_dir`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZHnyIKiylM-"
      },
      "source": [
        "### .h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "J_mBUqiXxnE3"
      },
      "outputs": [],
      "source": [
        "model.save('./models/model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee8-ahpRyg8v"
      },
      "source": [
        "### .tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qTrgfoxXzOS_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Bahillo\\AppData\\Local\\Temp\\tmp2b0magaa\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Bahillo\\AppData\\Local\\Temp\\tmp2b0magaa\\assets\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "33655512"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the model to the TensorFlow Lite format\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# # USE QUANTIZATION\n",
        "# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"./models/model.tflite\", \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6wAcl70z9O-"
      },
      "source": [
        "#### Optional\n",
        "For edge devices, see [supported platforms](https://www.tensorflow.org/lite/microcontrollers#supported_platforms)\n",
        "\n",
        "If the device requires to use C header file (.h) ...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KLBzMPZr0UMn"
      },
      "outputs": [],
      "source": [
        "def hex_to_c_array(hex_data) -> str:\n",
        "    # Declare C variable\n",
        "    c_str = 'unsigned char model[] = {'\n",
        "    hex_array = []\n",
        "\n",
        "    for i, val in enumerate(hex_data):\n",
        "        # Construct string from hex\n",
        "        hex_str = format(val, '#04x')\n",
        "\n",
        "        # Add formatting so each line stays within 80 characters\n",
        "        if (i + 1) < len(hex_data):\n",
        "            hex_str += ','\n",
        "        if (i + 1) % 12 == 0:\n",
        "            hex_str += '\\n'\n",
        "        hex_array.append(hex_str)\n",
        "\n",
        "    # Wrapping up\n",
        "    c_str += '\\n ' + format(' '.join(hex_array)) + '};'\n",
        "\n",
        "    return c_str\n",
        "\n",
        "with open('./models/model.h', 'w') as f:\n",
        "    content = hex_to_c_array(tflite_model)\n",
        "    f.write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCIXcdxZ41oK"
      },
      "source": [
        "## Test new images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GjrB0MP6LCH"
      },
      "source": [
        "### Load the model using the **.h5** file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0-7F-pdB481o"
      },
      "outputs": [],
      "source": [
        "loaded_model =  tf.keras.models.load_model('./models/model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "HBkFBJAD55XH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 60ms/step\n",
            "[2.5089437e-03 3.4435138e-10 2.5526322e-07 9.9749076e-01]\n",
            "This image most likely belongs to Yellow Sigatoka with a 99.75 percent confidence.\n"
          ]
        }
      ],
      "source": [
        "img = tf.keras.utils.load_img('yellowsigatokatest.jpg', target_size=IMAGE_SIZE) # replace with your file name here\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array/255, 0) # Scale from 0 to 1 and create a batch\n",
        "\n",
        "yhat = model.predict(img_array)\n",
        "score = yhat[0]\n",
        "\n",
        "print(score)\n",
        "\n",
        "print(\n",
        "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_MPdkFJ7gcRQ",
        "NqWbwfAMu6h-",
        "mUzIcUJ-u0p2",
        "7uRAHMkGvPb-"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a0a4648fbe25e095c7638071d099d3f417a477c0b71c0c2212dbfe58fdfe9b4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
