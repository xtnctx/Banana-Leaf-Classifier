{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "This may take a long time ... ~ 20 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6MzSkp0y435"
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3NmZDq3Xs1H"
   },
   "source": [
    "## Upload data\n",
    "\n",
    "Group images by separating each class into one folder then wrap all the folder into another folder.\n",
    "\n",
    "Ex.\n",
    "```\n",
    "data/ \n",
    "  │\n",
    "  └─── class1/\n",
    "  │        │\n",
    "  |        └─── image1.png\n",
    "  │        └─── image2.jpg\n",
    "  |        └─── ...\n",
    "  │   \n",
    "  └─── class2/\n",
    "  │        │\n",
    "  |        └─── image123.png\n",
    "  │        └─── image456.jpg\n",
    "  |        └─── ...\n",
    "  |\n",
    "  └─── .../\n",
    "           │\n",
    "           └─── ...\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opf5mi7TaBkz"
   },
   "source": [
    "## Remove unecessary files\n",
    "\n",
    "Image file extension that are only acceptable are selected in the `image_exts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjWJCi2sbIBn"
   },
   "outputs": [],
   "source": [
    "import imghdr, os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYsl0Bo8YXi1"
   },
   "outputs": [],
   "source": [
    "data_dir = './Banana Leaf Disease'\n",
    "image_exts = ['jpeg', 'jpg', 'bmp', 'png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyQaeAa2a14o"
   },
   "outputs": [],
   "source": [
    "nr = 0\n",
    "for image_class in os.listdir(data_dir):\n",
    "  for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "    image_path = os.path.join(data_dir, image_class, image)\n",
    "    try:\n",
    "      tip = imghdr.what(image_path)\n",
    "      if tip not in image_exts:\n",
    "        print(f'Image not in ext list {image_path}')\n",
    "        os.remove(image_path)\n",
    "        nr += 1\n",
    "    except Exception as e:\n",
    "        print(f'Issue with image {image_path}')\n",
    "\n",
    "print(f'Removed {nr} unwanted files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure settings\n",
    "\n",
    "Apply all the settings here for preprocessing, building, and training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (256, 256) # Square sized are recommended for stability\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "KERNEL_SIZE = (3, 3)\n",
    "STRIDES = 1\n",
    "\n",
    "# Ratio for splitting dataset\n",
    "TRAIN_VAL = 0.8\n",
    "VALID_VAL = 0.2\n",
    "assert TRAIN_VAL + VALID_VAL == 1, 'Ratio must be added to 100%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzGOzfkSc0Fl"
   },
   "source": [
    "## Prepare and randomize the images\n",
    "\n",
    "Randomly split input and output pairs into sets of data: 80% for training, 20% for validation.\n",
    "\n",
    "  - the training set is used to train the model\n",
    "  - the validation set is used to measure how well the model is performing during training\n",
    "\n",
    "Using the `tf.keras.utils.image_dataset_from_directory`, we can prepare our dataset easily.\n",
    "\n",
    "See https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RTO-B8jfZ3v"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTgo-RYGfR_X"
   },
   "outputs": [],
   "source": [
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split = 1-TRAIN_VAL, # reserve for validation\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=IMAGE_SIZE,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=True)\n",
    "\n",
    "val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=VALID_VAL,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=IMAGE_SIZE,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=True)\n",
    "\n",
    "class_names = train_data.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_data.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_data:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the dataset for performance\n",
    "\n",
    "* [Dataset.cache](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
    "\n",
    "* [Dataset.prefetch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) overlaps data preprocessing and model execution while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "Data augmentation takes the approach of generating additional training data from your existing examples by augmenting them using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential(\n",
    "  [\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\", input_shape=IMAGE_SIZE+(3,)),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_data.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data\n",
    "\n",
    "The RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general you should seek to make your input values small.\n",
    "\n",
    "Here, you will standardize values to be in the [0, 1] range by using [tf.keras.layers.Rescaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8M7urx0srNR"
   },
   "source": [
    "## Build & Train the Model\n",
    "\n",
    "Build and train a [TensorFlow](https://www.tensorflow.org) model using the high-level [Keras](https://www.tensorflow.org/guide/keras) API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqWbwfAMu6h-"
   },
   "source": [
    "### Build the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJGqDkXMs17t"
   },
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(data_augmentation)\n",
    "model.add(normalization_layer)\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(16, KERNEL_SIZE, STRIDES, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, KERNEL_SIZE, STRIDES, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64, KERNEL_SIZE, STRIDES, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(num_classes, name=\"outputs\"))\n",
    "\n",
    "\n",
    "model.compile('adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUzIcUJ-u0p2"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkG48YX87-b4"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_data, epochs=EPOCHS, validation_data=val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training results\n",
    "Create plots of the loss and accuracy on the training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLHwnChCxoco"
   },
   "source": [
    "## Save the model\n",
    "\n",
    " To test different brand new images that are not in the `data_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZHnyIKiylM-"
   },
   "source": [
    "### .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_mBUqiXxnE3"
   },
   "outputs": [],
   "source": [
    "model.save('./models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee8-ahpRyg8v"
   },
   "source": [
    "### .tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTrgfoxXzOS_"
   },
   "outputs": [],
   "source": [
    "# Convert the model to the TensorFlow Lite format\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# # USE QUANTIZATION\n",
    "# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"./models/model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6wAcl70z9O-"
   },
   "source": [
    "#### Optional\n",
    "For edge devices, see [supported platforms](https://www.tensorflow.org/lite/microcontrollers#supported_platforms)\n",
    "\n",
    "If the device requires to use C header file (.h) ...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLBzMPZr0UMn"
   },
   "outputs": [],
   "source": [
    "def hex_to_c_array(hex_data) -> str:\n",
    "    # Declare C variable\n",
    "    c_str = 'unsigned char model[] = {'\n",
    "    hex_array = []\n",
    "\n",
    "    for i, val in enumerate(hex_data):\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n'\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Wrapping up\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '};'\n",
    "\n",
    "    return c_str\n",
    "\n",
    "with open('./models/model.h', 'w') as f:\n",
    "    content = hex_to_c_array(tflite_model)\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCIXcdxZ41oK"
   },
   "source": [
    "## Test new images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GjrB0MP6LCH"
   },
   "source": [
    "### Load the model using the **.h5** file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-7F-pdB481o"
   },
   "outputs": [],
   "source": [
    "loaded_model =  tf.keras.models.load_model('./models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBkFBJAD55XH"
   },
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img('yellowsigatokatest.jpg', target_size=IMAGE_SIZE) # replace with your file name here\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # create a batch\n",
    "\n",
    "yhat = model.predict(img_array)\n",
    "score = tf.nn.softmax(yhat[0])\n",
    "\n",
    "print(score)\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the **.tflite** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_MODEL_FILE_PATH = './models/model.tflite' # The default path to the saved TensorFlow Lite model\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TF_MODEL_FILE_PATH)\n",
    "\n",
    "classify_lite = interpreter.get_signature_runner('serving_default')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img('yellowsigatokatest.jpg', target_size=IMAGE_SIZE) # replace with your file name here\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # create a batch\n",
    "\n",
    "print(score)\n",
    "\n",
    "predictions_lite = classify_lite(sequential_input=img_array)['outputs']\n",
    "score_lite = tf.nn.softmax(predictions_lite)\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score_lite)], 100 * np.max(score_lite))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_MPdkFJ7gcRQ",
    "NqWbwfAMu6h-",
    "mUzIcUJ-u0p2",
    "7uRAHMkGvPb-"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a0a4648fbe25e095c7638071d099d3f417a477c0b71c0c2212dbfe58fdfe9b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
