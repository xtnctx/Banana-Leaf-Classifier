{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6MzSkp0y435"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3NmZDq3Xs1H"
      },
      "source": [
        "## Upload data\n",
        "\n",
        "Group images by separating each class into one folder then wrap all the folder into another folder.\n",
        "\n",
        "Ex.\n",
        "```\n",
        "data/ \n",
        "  │\n",
        "  └─── class1/\n",
        "  │        │\n",
        "  |        └─── image1.png\n",
        "  │        └─── image2.jpg\n",
        "  |        └─── ...\n",
        "  │   \n",
        "  └─── class2/\n",
        "  │        │\n",
        "  |        └─── image123.png\n",
        "  │        └─── image456.jpg\n",
        "  |        └─── ...\n",
        "  |\n",
        "  └─── .../\n",
        "           │\n",
        "           └─── ...\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opf5mi7TaBkz"
      },
      "source": [
        "## Remove unecessary files\n",
        "\n",
        "Image file extension that are only acceptable are selected in the `image_exts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mjWJCi2sbIBn"
      },
      "outputs": [],
      "source": [
        "import cv2, imghdr, os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pYsl0Bo8YXi1"
      },
      "outputs": [],
      "source": [
        "data_dir = './data'\n",
        "image_exts = ['jpeg', 'jpg', 'bmp', 'png']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pyQaeAa2a14o"
      },
      "outputs": [],
      "source": [
        "for image_class in os.listdir(data_dir):\n",
        "  for image in os.listdir(os.path.join(data_dir, image_class)):\n",
        "    image_path = os.path.join(data_dir, image_class, image)\n",
        "    try:\n",
        "      img = cv2.imread(image_path)\n",
        "      tip = imghdr.what(image_path)\n",
        "      if tip not in image_exts:\n",
        "        print(f'Image not in ext list {image_path}')\n",
        "        os.remove(image_path)\n",
        "    except Exception as e:\n",
        "            print(f'Issue with image {image_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure settings\n",
        "\n",
        "Apply all the settings here for preprocessing, building, and training the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (256, 256) # Square sized are recommended for stability\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = ...\n",
        "KERNEL_SIZE = (3, 3)\n",
        "STRIDES = 1\n",
        "\n",
        "# Ratio for splitting dataset\n",
        "TRAIN_VAL = 0.7\n",
        "VALID_VAL = 0.2\n",
        "TEST_VAL = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzGOzfkSc0Fl"
      },
      "source": [
        "## Prepare, randomize, and normalize the images\n",
        "\n",
        "Using the `tf.keras.utils.image_dataset_from_directory` with image_size of (256, 256).\n",
        "\n",
        "See https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_RTO-B8jfZ3v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TTgo-RYGfR_X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 171 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "data = tf.keras.utils.image_dataset_from_directory(data_dir, image_size=IMAGE_SIZE, shuffle=True) # Images are resized and shuffled\n",
        "class_names = data.class_names\n",
        "data = data.map(lambda x, y : (x/255, y)) # Normalize data between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 256, 256, 3)\n",
            "(32,)\n"
          ]
        }
      ],
      "source": [
        "for image_batch, labels_batch in data:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MPdkFJ7gcRQ"
      },
      "source": [
        "## Split the input and output pairs for training\n",
        "\n",
        "Randomly split input and output pairs into sets of data: 70% for training, 20% for validation, and 10% for testing.\n",
        "\n",
        "  - the training set is used to train the model\n",
        "  - the validation set is used to measure how well the model is performing during training\n",
        "  - the testing set is used to test the model after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hZUelmtUm6H-"
      },
      "outputs": [],
      "source": [
        "train_size = round(len(data)*TRAIN_VAL)\n",
        "val_size = round(len(data)*VALID_VAL)\n",
        "test_size = round(len(data)*TEST_VAL)\n",
        "assert train_size + val_size + test_size == len(data), f'sum must be {len(data)}'\n",
        "\n",
        "train = data.take(train_size)\n",
        "val = data.skip(train_size).take(val_size)\n",
        "test = data.skip(train_size + val_size).take(test_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure the dataset for performance\n",
        "\n",
        "* [Dataset.cache](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
        "\n",
        "* [Dataset.prefetch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) overlaps data preprocessing and model execution while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val = val.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test = test.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8M7urx0srNR"
      },
      "source": [
        "## Build & Train the Model\n",
        "\n",
        "Build and train a [TensorFlow](https://www.tensorflow.org) model using the high-level [Keras](https://www.tensorflow.org/guide/keras) API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqWbwfAMu6h-"
      },
      "source": [
        "### Build the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BJGqDkXMs17t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 256, 256, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 128, 128, 16)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 128, 128, 32)      4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 64, 64, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 64, 64, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 32, 32, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 65536)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               8388736   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,412,578\n",
            "Trainable params: 8,412,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(16, KERNEL_SIZE, STRIDES, padding='same', activation='relu', input_shape=IMAGE_SIZE+(3,)))\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(32, KERNEL_SIZE, STRIDES, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64, KERNEL_SIZE, STRIDES, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile('adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUzIcUJ-u0p2"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lkG48YX87-b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.4371e-04 - accuracy: 1.0000 - val_loss: 0.3768 - val_accuracy: 0.9062\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 6s 2s/step - loss: 1.3649e-04 - accuracy: 1.0000 - val_loss: 0.3837 - val_accuracy: 0.9062\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.3128e-04 - accuracy: 1.0000 - val_loss: 0.3893 - val_accuracy: 0.9062\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.2620e-04 - accuracy: 1.0000 - val_loss: 0.3820 - val_accuracy: 0.9062\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.1879e-04 - accuracy: 1.0000 - val_loss: 0.3846 - val_accuracy: 0.9062\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.1324e-04 - accuracy: 1.0000 - val_loss: 0.3850 - val_accuracy: 0.9062\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.0884e-04 - accuracy: 1.0000 - val_loss: 0.3785 - val_accuracy: 0.9062\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.0370e-04 - accuracy: 1.0000 - val_loss: 0.3794 - val_accuracy: 0.9062\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.0010e-04 - accuracy: 1.0000 - val_loss: 0.3772 - val_accuracy: 0.9062\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 9.6757e-05 - accuracy: 1.0000 - val_loss: 0.3781 - val_accuracy: 0.9062\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 9.2665e-05 - accuracy: 1.0000 - val_loss: 0.3788 - val_accuracy: 0.9062\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 8.9536e-05 - accuracy: 1.0000 - val_loss: 0.3864 - val_accuracy: 0.9062\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 8.5876e-05 - accuracy: 1.0000 - val_loss: 0.3904 - val_accuracy: 0.9062\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 8.4098e-05 - accuracy: 1.0000 - val_loss: 0.3865 - val_accuracy: 0.9062\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 8.0527e-05 - accuracy: 1.0000 - val_loss: 0.3851 - val_accuracy: 0.9062\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 7.8152e-05 - accuracy: 1.0000 - val_loss: 0.3859 - val_accuracy: 0.9062\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 7.5107e-05 - accuracy: 1.0000 - val_loss: 0.3894 - val_accuracy: 0.9062\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 7.3552e-05 - accuracy: 1.0000 - val_loss: 0.3869 - val_accuracy: 0.9062\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 7.0537e-05 - accuracy: 1.0000 - val_loss: 0.3889 - val_accuracy: 0.9062\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 5s 1s/step - loss: 6.8794e-05 - accuracy: 1.0000 - val_loss: 0.3876 - val_accuracy: 0.9062\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train, epochs=20, validation_data=val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uRAHMkGvPb-"
      },
      "source": [
        "## Evaluate with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 314ms/step\n",
            "+---------------+------------+----------+\n",
            "|   Predictions |   Expected | Result   |\n",
            "|---------------+------------+----------|\n",
            "|             0 |          0 | True     |\n",
            "|             0 |          1 | False    |\n",
            "|             0 |          0 | True     |\n",
            "|             0 |          0 | True     |\n",
            "|             0 |          0 | True     |\n",
            "|             0 |          0 | True     |\n",
            "|             0 |          0 | True     |\n",
            "|             0 |          0 | True     |\n",
            "|             0 |          0 | True     |\n",
            "|             1 |          1 | True     |\n",
            "|             0 |          0 | True     |\n",
            "+---------------+------------+----------+\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.3440 - accuracy: 0.9091\n",
            "Model loss (Test set): 0.34396740794181824\n",
            "Model Accuracy (Test set): 0.9090909361839294\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "points = test.map(lambda x, y: x)\n",
        "labels = test.map(lambda x, y: y)\n",
        "test_yhat = model.predict(points)\n",
        "test_yhat = tf.math.argmax(test_yhat, -1)\n",
        "\n",
        "# print the predictions and the expected ouputs\n",
        "result = list(zip(np.array(test_yhat), list(labels.as_numpy_iterator())[0]))\n",
        "delta = [True if elem[0] == elem[1] else False for elem in result]\n",
        "table = list(zip(*zip(*result), delta))\n",
        "\n",
        "print (tabulate(table, headers=[\"Predictions\", \"Expected\", \"Result\"], tablefmt=\"psql\"))\n",
        "\n",
        "loss, acc = model.evaluate(test)\n",
        "print(f'Model loss (Test set): {loss}')\n",
        "print(f'Model Accuracy (Test set): {acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLHwnChCxoco"
      },
      "source": [
        "## Save the model\n",
        "\n",
        " To test different brand new images that are not in the `data_dir`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZHnyIKiylM-"
      },
      "source": [
        "### .h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "J_mBUqiXxnE3"
      },
      "outputs": [],
      "source": [
        "model.save('./models/model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee8-ahpRyg8v"
      },
      "source": [
        "### .tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qTrgfoxXzOS_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Bahillo\\AppData\\Local\\Temp\\tmpc7mjmn8s\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Bahillo\\AppData\\Local\\Temp\\tmpc7mjmn8s\\assets\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "33654376"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the model to the TensorFlow Lite format\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# # USE QUANTIZATION\n",
        "# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"./models/model.tflite\", \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6wAcl70z9O-"
      },
      "source": [
        "#### Optional\n",
        "For edge devices, see [supported platforms](https://www.tensorflow.org/lite/microcontrollers#supported_platforms)\n",
        "\n",
        "If the device requires to use C header file (.h) ...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KLBzMPZr0UMn"
      },
      "outputs": [],
      "source": [
        "def hex_to_c_array(hex_data) -> str:\n",
        "    # Declare C variable\n",
        "    c_str = 'unsigned char model[] = {'\n",
        "    hex_array = []\n",
        "\n",
        "    for i, val in enumerate(hex_data):\n",
        "        # Construct string from hex\n",
        "        hex_str = format(val, '#04x')\n",
        "\n",
        "        # Add formatting so each line stays within 80 characters\n",
        "        if (i + 1) < len(hex_data):\n",
        "            hex_str += ','\n",
        "        if (i + 1) % 12 == 0:\n",
        "            hex_str += '\\n'\n",
        "        hex_array.append(hex_str)\n",
        "\n",
        "    # Wrapping up\n",
        "    c_str += '\\n ' + format(' '.join(hex_array)) + '};'\n",
        "\n",
        "    return c_str\n",
        "\n",
        "with open('./models/model.h', 'w') as f:\n",
        "    content = hex_to_c_array(tflite_model)\n",
        "    f.write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCIXcdxZ41oK"
      },
      "source": [
        "## Test new images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GjrB0MP6LCH"
      },
      "source": [
        "### Load the model using the **.h5** file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "0-7F-pdB481o"
      },
      "outputs": [],
      "source": [
        "loaded_model =  tf.keras.models.load_model('./models/model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "HBkFBJAD55XH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 51ms/step\n",
            "[1.000000e+00 7.235012e-10]\n",
            "This image most likely belongs to happy with a 100.00 percent confidence.\n"
          ]
        }
      ],
      "source": [
        "img = tf.keras.utils.load_img('happytest.jpg', target_size=IMAGE_SIZE) # replace with your file name here\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array/255, 0) # Scale from 0 to 1 and create a batch\n",
        "\n",
        "yhat = model.predict(img_array)\n",
        "score = yhat[0]\n",
        "\n",
        "print(score)\n",
        "\n",
        "print(\n",
        "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# if yhat > 0.5:\n",
        "#     print('Predicted class is Sad')\n",
        "# else:\n",
        "#     print('Predicted class is Happy')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_MPdkFJ7gcRQ",
        "NqWbwfAMu6h-",
        "mUzIcUJ-u0p2",
        "7uRAHMkGvPb-"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a0a4648fbe25e095c7638071d099d3f417a477c0b71c0c2212dbfe58fdfe9b4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
