{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6MzSkp0y435"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3NmZDq3Xs1H"
      },
      "source": [
        "## Upload data\n",
        "\n",
        "Group images by separating each class into one folder then wrap all the folder another folder named `data`.\n",
        "\n",
        "Ex.\n",
        "```\n",
        "data/ \n",
        "  │\n",
        "  └─── class1/\n",
        "  │        │\n",
        "  |        └─── image1.png\n",
        "  │        └─── image2.jpg\n",
        "  |        └─── ...\n",
        "  │   \n",
        "  └─── class2/\n",
        "  │        │\n",
        "  |        └─── image123.png\n",
        "  │        └─── image456.jpg\n",
        "  |        └─── ...\n",
        "  |\n",
        "  └─── .../\n",
        "           │\n",
        "           └─── ...\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opf5mi7TaBkz"
      },
      "source": [
        "## Remove unecessary files\n",
        "\n",
        "Image file extension that are only acceptable are selected in the `image_exts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mjWJCi2sbIBn"
      },
      "outputs": [],
      "source": [
        "import cv2, imghdr, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pYsl0Bo8YXi1"
      },
      "outputs": [],
      "source": [
        "data_dir = 'data'\n",
        "image_exts = ['jpeg', 'jpg', 'bmp', 'png']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pyQaeAa2a14o"
      },
      "outputs": [],
      "source": [
        "for image_class in os.listdir(data_dir):\n",
        "  for image in os.listdir(os.path.join(data_dir, image_class)):\n",
        "    image_path = os.path.join(data_dir, image_class, image)\n",
        "    try:\n",
        "      img = cv2.imread(image_path)\n",
        "      tip = imghdr.what(image_path)\n",
        "      if tip not in image_exts:\n",
        "        print(f'Image not in ext list {image_path}')\n",
        "        os.remove(image_path)\n",
        "    except Exception as e:\n",
        "            print(f'Issue with image {image_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzGOzfkSc0Fl"
      },
      "source": [
        "## Prepare, randomize, and normalize the images\n",
        "\n",
        "Using the `tf.keras.utils.image_dataset_from_directory` with image_size of (256, 256).\n",
        "\n",
        "See https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_RTO-B8jfZ3v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TTgo-RYGfR_X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 171 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "data = tf.keras.utils.image_dataset_from_directory('data', image_size=(256, 256))\n",
        "data = data.map(lambda x, y : (x/255, y)) # Normalize data between 0 and 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MPdkFJ7gcRQ"
      },
      "source": [
        "## Split the input and output pairs for training\n",
        "\n",
        "Randomly split input and output pairs into sets of data: 70% for training, 20% for validation, and 10% for testing.\n",
        "\n",
        "  - the training set is used to train the model\n",
        "  - the validation set is used to measure how well the model is performing during training\n",
        "  - the testing set is used to test the model after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hZUelmtUm6H-"
      },
      "outputs": [],
      "source": [
        "train_size = round(len(data)*0.7)\n",
        "val_size = round(len(data)*0.2)\n",
        "test_size = round(len(data)*0.1)\n",
        "assert train_size + val_size + test_size == len(data), f'sum must be {len(data)}'\n",
        "\n",
        "train = data.take(train_size)\n",
        "val = data.skip(train_size).take(val_size)\n",
        "test = data.skip(train_size + val_size).take(test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8M7urx0srNR"
      },
      "source": [
        "## Build & Train the Model\n",
        "\n",
        "Build and train a [TensorFlow](https://www.tensorflow.org) model using the high-level [Keras](https://www.tensorflow.org/guide/keras) API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqWbwfAMu6h-"
      },
      "source": [
        "### Build the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BJGqDkXMs17t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 127, 127, 16)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 32)      4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 62, 62, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 16)        4624      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 14400)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               3686656   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,696,625\n",
            "Trainable params: 3,696,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(16, (3, 3), 1, activation='relu', input_shape=(256, 256, 3)))\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(32, (3, 3), 1, activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(16, (3, 3), 1, activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUzIcUJ-u0p2"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lkG48YX87-b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 9s 2s/step - loss: 0.8271 - accuracy: 0.5859 - val_loss: 0.8447 - val_accuracy: 0.4062\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.6985 - accuracy: 0.6406 - val_loss: 0.6913 - val_accuracy: 0.5625\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.6269 - accuracy: 0.6172 - val_loss: 0.5917 - val_accuracy: 0.6562\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.5479 - accuracy: 0.7266 - val_loss: 0.4991 - val_accuracy: 0.7500\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.4770 - accuracy: 0.7578 - val_loss: 0.4373 - val_accuracy: 0.8438\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.4046 - accuracy: 0.8125 - val_loss: 0.2979 - val_accuracy: 0.9062\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.3878 - accuracy: 0.8203 - val_loss: 0.3436 - val_accuracy: 0.8125\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.2983 - accuracy: 0.8672 - val_loss: 0.2064 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.2313 - accuracy: 0.9375 - val_loss: 0.1645 - val_accuracy: 0.9688\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.1845 - accuracy: 0.9688 - val_loss: 0.1574 - val_accuracy: 0.9375\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.1078 - accuracy: 0.9922 - val_loss: 0.0923 - val_accuracy: 0.9688\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.1074 - accuracy: 0.9766 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 9s 2s/step - loss: 0.0950 - accuracy: 0.9688 - val_loss: 0.0630 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.0763 - accuracy: 0.9844 - val_loss: 0.0659 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 9s 2s/step - loss: 0.0383 - accuracy: 0.9922 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 9s 2s/step - loss: 0.0267 - accuracy: 0.9922 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train, epochs=20, validation_data=val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uRAHMkGvPb-"
      },
      "source": [
        "## Run with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SneJudV7vj7W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 267ms/step\n",
            "Precision: 1.0, Recall: 1.0, Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "pre = tf.keras.metrics.Precision()\n",
        "re = tf.keras.metrics.Recall()\n",
        "acc = tf.keras.metrics.BinaryAccuracy()\n",
        "\n",
        "for batch in test.as_numpy_iterator():\n",
        "    X, y = batch\n",
        "    yhat = model.predict(X)\n",
        "    pre.update_state(y, yhat)\n",
        "    re.update_state(y, yhat)\n",
        "    acc.update_state(y, yhat)\n",
        "    \n",
        "print(f'Precision: {pre.result().numpy()}, Recall: {re.result().numpy()}, Accuracy: {acc.result().numpy()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLHwnChCxoco"
      },
      "source": [
        "## Save the model\n",
        "\n",
        " To test different brand new images that are not in the `data_dir`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZHnyIKiylM-"
      },
      "source": [
        "### .h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J_mBUqiXxnE3"
      },
      "outputs": [],
      "source": [
        "model.save('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee8-ahpRyg8v"
      },
      "source": [
        "### .tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qTrgfoxXzOS_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Bahillo\\AppData\\Local\\Temp\\tmp7r32hj4b\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Bahillo\\AppData\\Local\\Temp\\tmp7r32hj4b\\assets\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "14790588"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the model to the TensorFlow Lite format\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# # USE QUANTIZATION\n",
        "# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"model.tflite\", \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6wAcl70z9O-"
      },
      "source": [
        "#### Optional\n",
        "For edge devices, see [supported platforms](https://www.tensorflow.org/lite/microcontrollers#supported_platforms)\n",
        "\n",
        "If the device requires to use C header file (.h) ...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KLBzMPZr0UMn"
      },
      "outputs": [],
      "source": [
        "def hex_to_c_array(hex_data) -> str:\n",
        "    # Declare C variable\n",
        "    c_str = 'unsigned char model[] = {'\n",
        "    hex_array = []\n",
        "\n",
        "    for i, val in enumerate(hex_data):\n",
        "        # Construct string from hex\n",
        "        hex_str = format(val, '#04x')\n",
        "\n",
        "        # Add formatting so each line stays within 80 characters\n",
        "        if (i + 1) < len(hex_data):\n",
        "            hex_str += ','\n",
        "        if (i + 1) % 12 == 0:\n",
        "            hex_str += '\\n'\n",
        "        hex_array.append(hex_str)\n",
        "\n",
        "    # Wrapping up\n",
        "    c_str += '\\n ' + format(' '.join(hex_array)) + '};'\n",
        "\n",
        "    return c_str\n",
        "\n",
        "with open('model.h', 'w') as f:\n",
        "    content = hex_to_c_array(tflite_model)\n",
        "    f.write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCIXcdxZ41oK"
      },
      "source": [
        "## Test new images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GjrB0MP6LCH"
      },
      "source": [
        "### Load the model using the **.h5** file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0-7F-pdB481o"
      },
      "outputs": [],
      "source": [
        "loaded_model =  tf.keras.models.load_model('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpDbP0Mg6PC3"
      },
      "source": [
        "### Begin prediction testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "HBkFBJAD55XH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 44ms/step\n",
            "\n",
            " [[0.00367736]]\n",
            "Predicted class is Happy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "img = cv2.imread('happytest.jpg') # replace with your file name here\n",
        "test_image = tf.image.resize(img, (256, 256))\n",
        "yhat = model.predict(np.expand_dims(test_image/255, 0))\n",
        "\n",
        "print('\\n', yhat)\n",
        "\n",
        "if yhat > 0.5:\n",
        "    print('Predicted class is Sad')\n",
        "else:\n",
        "    print('Predicted class is Happy')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_MPdkFJ7gcRQ",
        "NqWbwfAMu6h-",
        "mUzIcUJ-u0p2",
        "7uRAHMkGvPb-"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a0a4648fbe25e095c7638071d099d3f417a477c0b71c0c2212dbfe58fdfe9b4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
